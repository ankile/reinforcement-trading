{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"reinforcement_crypto-6-4-small.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1rp7MVBTEGUVKnnP56pg9CsxwpzEA6kp9","authorship_tag":"ABX9TyPaC1Dn2SdH2mnFfn42SqtQ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"w0b2-3J6_1EN","colab_type":"code","outputId":"cff53c9a-6c5b-4e5f-b0bd-04f110f326a5","executionInfo":{"status":"ok","timestamp":1587741470126,"user_tz":-120,"elapsed":4199,"user":{"displayName":"Lars Lien Ankile","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgTC8zAmM3XfInMioErd3CkpZJzfok6A85YroYBSQ=s64","userId":"12768464313641053119"}},"colab":{"base_uri":"https://localhost:8080/","height":306}},"source":["gpu_info = !nvidia-smi\n","gpu_info = '\\n'.join(gpu_info)\n","if gpu_info.find('failed') >= 0:\n","  print('Select the Runtime → \"Change runtime type\" menu to enable a GPU accelerator, ')\n","  print('and then re-execute this cell.')\n","else:\n","  print(gpu_info)"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Fri Apr 24 15:17:47 2020       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 440.64.00    Driver Version: 418.67       CUDA Version: 10.1     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|===============================+======================+======================|\n","|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   34C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                       GPU Memory |\n","|  GPU       PID   Type   Process name                             Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"6sFxUHA5OeSB","colab_type":"text"},"source":["# Reinforcement learning using DQN and CNN"]},{"cell_type":"markdown","metadata":{"id":"hV4jpowXZMeU","colab_type":"text"},"source":["### Improvement Ideas\n","\n","- Make the network deeper\n","- Normalize the Volume as well\n","- Make use of the extra features I have in the data\n","- Use LSTM model instead of CNN\n","    - That would require some rewriting of the gym so that it only feeds in one timestep observation at a time\n","- Train the network on multiple different crypto's and see if that helps it generalize for data it hasn't seen before"]},{"cell_type":"markdown","metadata":{"id":"qCSZuqQMQ0Xn","colab_type":"text"},"source":["## Training the Model"]},{"cell_type":"code","metadata":{"id":"oEuEPMLWEUqV","colab_type":"code","outputId":"35ddd2b1-780b-4879-848b-b703bfa39a51","executionInfo":{"status":"ok","timestamp":1587741610835,"user_tz":-120,"elapsed":144898,"user":{"displayName":"Lars Lien Ankile","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgTC8zAmM3XfInMioErd3CkpZJzfok6A85YroYBSQ=s64","userId":"12768464313641053119"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["!pip3 install ptan --quiet"],"execution_count":2,"outputs":[{"output_type":"stream","text":["\u001b[K     |████████████████████████████████| 773.1MB 23kB/s \n","\u001b[?25h  Building wheel for ptan (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[31mERROR: torchvision 0.5.0 has requirement torch==1.4.0, but you'll have torch 1.3.0 which is incompatible.\u001b[0m\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"zd00mGRbliVa","colab_type":"code","colab":{}},"source":["BASE_PATH = '/content/drive/My Drive/NTNU/6. Semester/NEURO140'"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"nZs-3ctOj7fw","colab_type":"code","colab":{}},"source":["%matplotlib notebook\n","import os\n","import gym\n","from gym import wrappers\n","import ptan\n","import argparse\n","import numpy as np\n","import pandas as pd\n","from datetime import datetime\n","\n","import torch\n","import torch.optim as optim\n","\n","os.chdir(os.path.join(BASE_PATH, 'agent1'))\n","\n","from lib import environ, models, data, common, validation\n","\n","from torch.utils.tensorboard import SummaryWriter"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Am_L1d1oXM5R","colab_type":"code","colab":{}},"source":["BATCH_SIZE = 32\n","\n","BARS_COUNT = 50\n","RESET_ON_CLOSE = True\n","RANDOM_OFS_ON_RESET = True\n","REWARD_ON_CLOSE = True\n","STATE_1D = True\n","VOLUMES = True\n","\n","TARGET_NET_SYNC = 1000\n","\n","# INTERVAL = '1M'\n","# PAIR = 'YNDX'\n","# DATA_FILENAME = 'YNDX_150101_151231.csv'\n","# DATA_PATH = os.path.join(BASE_PATH, \"Data\", INTERVAL, PAIR, DATA_FILENAME)\n","data_paths = {\n","    'BTCUSD': '/content/drive/My Drive/NTNU/6. Semester/NEURO140/Data/1M/BTCUSD/COINBASE_BTCUSD, 1.csv',\n","    'BTCEUR': '/content/drive/My Drive/NTNU/6. Semester/NEURO140/Data/1M/BTCEUR/COINBASE_BTCEUR, 1.csv',\n","    'BCHUSD': '/content/drive/My Drive/NTNU/6. Semester/NEURO140/Data/1M/BCHUSD/KRAKEN_BCHUSD, 1.csv',\n","    'EOSUSD': '/content/drive/My Drive/NTNU/6. Semester/NEURO140/Data/1M/EOSUSD/KRAKEN_EOSUSD, 1.csv',\n","    'LTCUSD': '/content/drive/My Drive/NTNU/6. Semester/NEURO140/Data/1M/LTCUSD/KRAKEN_LTCUSD, 1.csv',\n","    'XMRUSD': '/content/drive/My Drive/NTNU/6. Semester/NEURO140/Data/1M/XMRUSD/KRAKEN_XMRUSD, 1.csv',\n","    'XRPEUR': '/content/drive/My Drive/NTNU/6. Semester/NEURO140/Data/1M/XRPEUR/KRAKEN_XRPEUR, 1.csv',\n","    'XRPUSD': '/content/drive/My Drive/NTNU/6. Semester/NEURO140/Data/1M/XRPUSD/KRAKEN_XRPUSD, 1.csv',\n","    'XTZUSD': '/content/drive/My Drive/NTNU/6. Semester/NEURO140/Data/1M/XTZUSD/KRAKEN_XTZUSD, 1.csv',\n","}\n","\n","# VALIDATION_FILENAME = 'YNDX_160101_161231.csv'\n","# VALIDATION_PATH = os.path.join(BASE_PATH, \"Data\", INTERVAL, PAIR, VALIDATION_FILENAME)\n","validation_paths = {\n","    'ETHEUR': '/content/drive/My Drive/NTNU/6. Semester/NEURO140/Data/1M/ETHEUR/KRAKEN_ETHEUR, 1 (1).csv',\n","    'ETHUSD': '/content/drive/My Drive/NTNU/6. Semester/NEURO140/Data/1M/ETHUSD/KRAKEN_ETHUSD, 1 (3).csv',\n","}\n","\n","GAMMA = 0.99\n","\n","REPLAY_SIZE = 100_000\n","REPLAY_INITIAL = 10_000\n","\n","REWARD_STEPS = 2\n","\n","LEARNING_RATE = 0.0001\n","\n","STATES_TO_EVALUATE = 1000\n","EVAL_EVERY_STEP = 1000\n","\n","# EPSILON_START = 1.0\n","EPSILON_START = 1.0\n","EPSILON_STOP = 0.1\n","EPSILON_STEPS = 1_000_000\n","\n","CHECKPOINT_EVERY_STEP = 1_000_000\n","VALIDATION_EVERY_STEP = 100_000"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"T8xCArTCX7a0","colab_type":"code","colab":{}},"source":["# Helper function for creating the Prices-tuple\n","def get_tuple_from_df(df):\n","    return data.Prices(\n","        open=np.array(df['open']),\n","        high=np.array(df['high']),\n","        low=np.array(df['low']),\n","        close=np.array(df['close']),\n","        volume=np.array(df['Volume']),\n","    )"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9S8coQXbAwp3","colab_type":"code","colab":{}},"source":["def prices_to_relative(prices, normalize_volume=False):\n","    \"\"\"\n","    Convert prices to relative in respect to open price\n","    :param ochl: tuple with open, close, high, low\n","    :return: tuple with open, rel_close, rel_high, rel_low\n","    \"\"\"\n","    assert isinstance(prices, data.Prices)\n","    rh = (prices.high - prices.open) / prices.open\n","    rl = (prices.low - prices.open) / prices.open\n","    rc = (prices.close - prices.open) / prices.open\n","\n","    volume = prices.volume\n","    if normalize_volume:\n","        vol_cpy = prices.volume.copy()\n","        vol_cpy[0] = 0\n","\n","        for i, num in enumerate(volume[:-1], start=1):\n","            d = volume[i] - num\n","            if d == 0:\n","                vol_cpy[i] = 0\n","            elif num == 0:\n","                vol_cpy[i] = 1\n","            else:\n","                vol_cpy[i] = d / num\n","\n","        volume = vol_cpy\n","\n","    return data.Prices(open=prices.open, high=rh, low=rl, close=rc, volume=volume)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_C_4rQ_AR7pJ","colab_type":"code","colab":{}},"source":["# Get training data\n","stock_data = {}\n","for pair, path in data_paths.items():\n","    df = pd.read_csv(path)\n","    data_tuple = get_tuple_from_df(df)\n","    stock_data[pair] = prices_to_relative(data_tuple, normalize_volume=True)\n","\n","val_data = {}\n","for pair, path in validation_paths.items():\n","    df = pd.read_csv(path)\n","    data_tuple = get_tuple_from_df(df)\n","    val_data[pair] = prices_to_relative(data_tuple, normalize_volume=True)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"h_hkqpT-Deq5","colab_type":"code","colab":{}},"source":["import torch.nn as nn\n","import torch.nn.functional as F\n","\n","\n","class DQNConv1DLarge(nn.Module):\n","    def __init__(self, shape, actions_n):\n","        super(DQNConv1DLarge, self).__init__()\n","\n","        self.conv = nn.Sequential(\n","            nn.Conv1d(in_channels=shape[0], out_channels=64, kernel_size=3, padding=1),\n","            nn.MaxPool1d(3, 2),\n","            nn.ReLU(),\n","            nn.Dropout2d(p=0.1),\n","            nn.Conv1d(in_channels=64, out_channels=32, kernel_size=3, padding=1),\n","            nn.MaxPool1d(3, 2),\n","            nn.ReLU(),\n","            nn.Dropout2d(p=0.1),\n","            # nn.Conv1d(in_channels=64, out_channels=64, kernel_size=3, padding=1),\n","            # nn.MaxPool1d(3, 2),\n","            # nn.ReLU(),\n","            # nn.Dropout2d(p=0.1),\n","            nn.Conv1d(in_channels=32, out_channels=32, kernel_size=3, padding=1),\n","            nn.ReLU(),\n","            nn.Dropout2d(p=0.1),\n","            nn.Conv1d(in_channels=32, out_channels=32, kernel_size=3, padding=1),\n","            nn.ReLU(),\n","            nn.Dropout2d(p=0.1),\n","            nn.Conv1d(in_channels=32, out_channels=32, kernel_size=3, padding=1),\n","            nn.ReLU(),\n","        )\n","\n","        out_size = self._get_conv_out(shape)\n","\n","        self.fc_val = nn.Sequential(\n","            nn.Linear(out_size, 512),\n","            nn.ReLU(),\n","            nn.Dropout(),\n","            # nn.Linear(512, 512),\n","            # nn.ReLU(),\n","            # nn.Dropout2d(),\n","            nn.Linear(512, 1)\n","        )\n","\n","        self.fc_adv = nn.Sequential(\n","            nn.Linear(out_size, 512),\n","            nn.ReLU(),\n","            nn.Dropout(),\n","            # nn.Linear(512, 512),\n","            # nn.ReLU(),\n","            # nn.Dropout2d(),\n","            nn.Linear(512, actions_n)\n","        )\n","\n","    def _get_conv_out(self, shape):\n","        o = self.conv(torch.zeros(1, *shape))\n","        return int(np.prod(o.size()))\n","\n","    def forward(self, x):\n","        conv_out = self.conv(x).view(x.size()[0], -1)\n","        val = self.fc_val(conv_out)\n","        adv = self.fc_adv(conv_out)\n","        return val + adv - adv.mean(dim=1, keepdim=True)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"fvQrmVXab_zz","colab_type":"code","outputId":"b8528404-ff4f-47ba-f38d-551b30225eaf","executionInfo":{"status":"ok","timestamp":1587741663680,"user_tz":-120,"elapsed":197700,"user":{"displayName":"Lars Lien Ankile","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgTC8zAmM3XfInMioErd3CkpZJzfok6A85YroYBSQ=s64","userId":"12768464313641053119"}},"colab":{"base_uri":"https://localhost:8080/","height":391}},"source":["step_idx = 0\n","eval_states = None\n","best_mean_val = None\n","\n","# device = xm.xla_device()\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {device}\")\n","\n","run_name = \"test-6.4-crypto-agent-small\"  # + str(datetime.now())\n","\n","saves_path = os.path.join(BASE_PATH, \"agent1\", \"crypto-saves\", run_name)\n","print(f\"Saving to path:\\n{saves_path}\")\n","\n","os.makedirs(saves_path, exist_ok=True)\n","\n","env = environ.StocksEnv(\n","    stock_data,\n","    bars_count=BARS_COUNT,\n","    reset_on_close=RESET_ON_CLOSE,\n","    random_ofs_on_reset=RANDOM_OFS_ON_RESET,\n","    reward_on_close=REWARD_ON_CLOSE,\n","    state_1d=STATE_1D,\n","    volumes=VOLUMES,\n",")\n","\n","env_tst = environ.StocksEnv(\n","    stock_data,\n","    bars_count=BARS_COUNT,\n","    reset_on_close=RESET_ON_CLOSE,\n","    random_ofs_on_reset=RANDOM_OFS_ON_RESET,\n","    reward_on_close=REWARD_ON_CLOSE,\n","    state_1d=STATE_1D,\n","    volumes=VOLUMES,\n",")\n","\n","env = wrappers.TimeLimit(env, max_episode_steps=1000)\n","\n","env_val = environ.StocksEnv(\n","    val_data,\n","    bars_count=BARS_COUNT,\n","    reset_on_close=RESET_ON_CLOSE,\n","    random_ofs_on_reset=RANDOM_OFS_ON_RESET,\n","    reward_on_close=REWARD_ON_CLOSE,\n","    state_1d=STATE_1D,\n","    volumes=VOLUMES,\n",")\n","\n","writer = SummaryWriter(comment=\"-conv-\" + run_name + \"\")\n","net = models.DQNConv1D(env.observation_space.shape, env.action_space.n).to(device)\n","# Uncomment next line to use deeper CNN\n","# net = DQNConv1DLarge(env.observation_space.shape, env.action_space.n).to(device)\n","\n","print(net)\n","\n","tgt_net = ptan.agent.TargetNet(net)\n","selector = ptan.actions.EpsilonGreedyActionSelector(EPSILON_START)\n","agent = ptan.agent.DQNAgent(net, selector, device=device)\n","exp_source = ptan.experience.ExperienceSourceFirstLast(\n","    env, agent, GAMMA, steps_count=REWARD_STEPS\n",")\n","buffer = ptan.experience.ExperienceReplayBuffer(exp_source, REPLAY_SIZE)\n","optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE)\n","\n","\n","load_state = '/content/drive/My Drive/NTNU/6. Semester/NEURO140/agent1/crypto-saves/test-6.4-crypto-agent-small/checkpoint-32.data'\n","if load_state is not None:\n","    state = torch.load(load_state)\n","    net.load_state_dict(state['model_state_dict'])\n","    optimizer.load_state_dict(state['optimizer_state_dict'])\n","    step_idx = state['step_idx']\n","    best_mean_val = state['best_mean_val']\n","    print(f\"State loaded –> step index: {step_idx}, best mean val: {best_mean_val}\")\n","\n","    net.train()"],"execution_count":10,"outputs":[{"output_type":"stream","text":["Using device: cuda\n","Saving to path:\n","/content/drive/My Drive/NTNU/6. Semester/NEURO140/agent1/crypto-saves/test-6.4-crypto-agent-small\n","DQNConv1D(\n","  (conv): Sequential(\n","    (0): Conv1d(6, 128, kernel_size=(5,), stride=(1,))\n","    (1): ReLU()\n","    (2): Conv1d(128, 128, kernel_size=(5,), stride=(1,))\n","    (3): ReLU()\n","  )\n","  (fc_val): Sequential(\n","    (0): Linear(in_features=5376, out_features=512, bias=True)\n","    (1): ReLU()\n","    (2): Linear(in_features=512, out_features=1, bias=True)\n","  )\n","  (fc_adv): Sequential(\n","    (0): Linear(in_features=5376, out_features=512, bias=True)\n","    (1): ReLU()\n","    (2): Linear(in_features=512, out_features=3, bias=True)\n","  )\n",")\n","State loaded –> step index: 32000000, best mean val: 828585.9736392498\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"UvQ23LgpZsh9","colab_type":"code","outputId":"4c639294-cf1f-4a95-cfbb-3e39b7eb5385","executionInfo":{"status":"error","timestamp":1587752983700,"user_tz":-120,"elapsed":2856155,"user":{"displayName":"Lars Lien Ankile","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgTC8zAmM3XfInMioErd3CkpZJzfok6A85YroYBSQ=s64","userId":"12768464313641053119"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["# main training loop\n","with common.RewardTracker(writer, np.inf, group_rewards=100) as reward_tracker:\n","    while True:\n","        step_idx += 1\n","        buffer.populate(1)\n","        selector.epsilon = max(\n","            EPSILON_STOP, EPSILON_START - step_idx / EPSILON_STEPS\n","        )\n","\n","        new_rewards = exp_source.pop_rewards_steps()\n","        if new_rewards:\n","            reward_tracker.reward(new_rewards[0], step_idx, selector.epsilon)\n","\n","        if len(buffer) < REPLAY_INITIAL:\n","            continue\n","\n","        if eval_states is None:\n","            print(\"Initial buffer populated, start training\")\n","            eval_states = buffer.sample(STATES_TO_EVALUATE)\n","            eval_states = [\n","                np.array(transition.state, copy=False) for transition in eval_states\n","            ]\n","            eval_states = np.array(eval_states, copy=False)\n","\n","        if step_idx % EVAL_EVERY_STEP == 0:\n","            mean_val = common.calc_values_of_states(eval_states, net, device=device)\n","            writer.add_scalar(\"values_mean\", mean_val, step_idx)\n","            if best_mean_val is None or best_mean_val < mean_val:\n","                if best_mean_val is not None:\n","                    print(f\"{step_idx}: Best mean value updated {best_mean_val:.3f} -> {mean_val:.3f}\")\n","                best_mean_val = mean_val\n","                torch.save({\n","                                \"model_state_dict\": net.state_dict(),\n","                                \"optimizer_state_dict\": optimizer.state_dict(),\n","                                \"step_idx\": step_idx,\n","                                \"best_mean_val\": best_mean_val,\n","                            },\n","                            os.path.join(saves_path, f\"mean_val-{mean_val:.3f}.data\"))\n","                torch.save(net,\n","                            os.path.join(saves_path, f\"mean_val-{mean_val:.3f}-fullmodel.data\"))\n","\n","        optimizer.zero_grad()\n","        batch = buffer.sample(BATCH_SIZE)\n","        loss_v = common.calc_loss(\n","            batch, net, tgt_net.target_model, GAMMA ** REWARD_STEPS, device=device\n","        )\n","        loss_v.backward()\n","        optimizer.step()\n","\n","        if step_idx % TARGET_NET_SYNC == 0:\n","            tgt_net.sync()\n","\n","        if step_idx % CHECKPOINT_EVERY_STEP == 0:\n","            idx = step_idx // CHECKPOINT_EVERY_STEP\n","            torch.save({\n","                            \"model_state_dict\": net.state_dict(),\n","                            \"optimizer_state_dict\": optimizer.state_dict(),\n","                            \"step_idx\": step_idx,\n","                            \"best_mean_val\": best_mean_val,\n","                        },\n","                os.path.join(saves_path, f\"checkpoint-{idx}.data\"),\n","            )\n","            torch.save(net,\n","                os.path.join(saves_path, f\"fullmodel-{idx}.data\"),\n","            )\n","\n","        if step_idx % VALIDATION_EVERY_STEP == 0:\n","            res = validation.validation_run(env_tst, net, device=device)\n","            for key, val in res.items():\n","                writer.add_scalar(f\"{key.capitalize()}/train\", val, step_idx)\n","            res = validation.validation_run(env_val, net, device=device)\n","            for key, val in res.items():\n","                writer.add_scalar(f\"{key.capitalize()}/validate\", val, step_idx)\n","\n","print(\"Training done\")"],"execution_count":11,"outputs":[{"output_type":"stream","text":["32 003 602: done 100 games, mean reward -0.210, mean steps 36.01, speed 12296800.73 f/s, eps 0.10\n","32 006 568: done 200 games, mean reward -0.186, mean steps 32.84, speed 1362.48 f/s, eps 0.10\n","Initial buffer populated, start training\n","32 010 120: done 300 games, mean reward -0.178, mean steps 33.73, speed 1099.56 f/s, eps 0.10\n","32 016 893: done 400 games, mean reward -0.162, mean steps 42.23, speed 169.17 f/s, eps 0.10\n","32 022 618: done 500 games, mean reward -0.113, mean steps 45.23, speed 170.26 f/s, eps 0.10\n","32 027 278: done 600 games, mean reward -0.156, mean steps 45.46, speed 169.17 f/s, eps 0.10\n","32 030 615: done 700 games, mean reward -0.149, mean steps 43.73, speed 170.96 f/s, eps 0.10\n","32 035 753: done 800 games, mean reward -0.187, mean steps 44.69, speed 170.61 f/s, eps 0.10\n","32 040 037: done 900 games, mean reward -0.223, mean steps 44.48, speed 168.99 f/s, eps 0.10\n","32 045 359: done 1000 games, mean reward -0.243, mean steps 45.36, speed 170.50 f/s, eps 0.10\n","32 050 042: done 1100 games, mean reward -0.243, mean steps 45.49, speed 170.86 f/s, eps 0.10\n","32 054 361: done 1200 games, mean reward -0.246, mean steps 45.30, speed 170.20 f/s, eps 0.10\n","32 058 976: done 1300 games, mean reward -0.267, mean steps 45.37, speed 171.59 f/s, eps 0.10\n","32 064 700: done 1400 games, mean reward -0.265, mean steps 46.21, speed 171.23 f/s, eps 0.10\n","32 069 234: done 1500 games, mean reward -0.258, mean steps 46.16, speed 170.26 f/s, eps 0.10\n","32 074 288: done 1600 games, mean reward -0.269, mean steps 46.43, speed 170.38 f/s, eps 0.10\n","32 079 532: done 1700 games, mean reward -0.264, mean steps 46.78, speed 170.46 f/s, eps 0.10\n","32 085 425: done 1800 games, mean reward -0.262, mean steps 47.46, speed 170.75 f/s, eps 0.10\n","32 089 130: done 1900 games, mean reward -0.257, mean steps 46.91, speed 171.17 f/s, eps 0.10\n","32 094 021: done 2000 games, mean reward -0.227, mean steps 47.01, speed 169.70 f/s, eps 0.10\n","32 098 217: done 2100 games, mean reward -0.224, mean steps 46.77, speed 170.72 f/s, eps 0.10\n","32 104 214: done 2200 games, mean reward -0.237, mean steps 47.37, speed 78.36 f/s, eps 0.10\n","32 107 559: done 2300 games, mean reward -0.234, mean steps 46.76, speed 171.50 f/s, eps 0.10\n","32 112 101: done 2400 games, mean reward -0.231, mean steps 46.71, speed 170.63 f/s, eps 0.10\n","32 115 370: done 2500 games, mean reward -0.230, mean steps 46.15, speed 170.34 f/s, eps 0.10\n","32 119 244: done 2600 games, mean reward -0.228, mean steps 45.86, speed 170.32 f/s, eps 0.10\n","32 123 322: done 2700 games, mean reward -0.228, mean steps 45.67, speed 170.87 f/s, eps 0.10\n","32 127 398: done 2800 games, mean reward -0.217, mean steps 45.50, speed 170.72 f/s, eps 0.10\n","32 132 592: done 2900 games, mean reward -0.216, mean steps 45.72, speed 170.45 f/s, eps 0.10\n","32 137 040: done 3000 games, mean reward -0.214, mean steps 45.68, speed 170.84 f/s, eps 0.10\n","32 142 162: done 3100 games, mean reward -0.206, mean steps 45.86, speed 169.10 f/s, eps 0.10\n","32 146 213: done 3200 games, mean reward -0.204, mean steps 45.69, speed 170.15 f/s, eps 0.10\n","32 151 424: done 3300 games, mean reward -0.210, mean steps 45.89, speed 170.26 f/s, eps 0.10\n","32 155 146: done 3400 games, mean reward -0.207, mean steps 45.63, speed 170.61 f/s, eps 0.10\n","32 158 737: done 3500 games, mean reward -0.210, mean steps 45.35, speed 170.46 f/s, eps 0.10\n","32 161 778: done 3600 games, mean reward -0.207, mean steps 44.94, speed 171.00 f/s, eps 0.10\n","32 165 887: done 3700 games, mean reward -0.207, mean steps 44.83, speed 170.87 f/s, eps 0.10\n","32 170 090: done 3800 games, mean reward -0.211, mean steps 44.76, speed 170.41 f/s, eps 0.10\n","32 173 433: done 3900 games, mean reward -0.214, mean steps 44.47, speed 170.64 f/s, eps 0.10\n","32 177 821: done 4000 games, mean reward -0.217, mean steps 44.45, speed 171.33 f/s, eps 0.10\n","32 182 789: done 4100 games, mean reward -0.216, mean steps 44.58, speed 170.54 f/s, eps 0.10\n","32 187 364: done 4200 games, mean reward -0.218, mean steps 44.61, speed 170.57 f/s, eps 0.10\n","32 192 398: done 4300 games, mean reward -0.220, mean steps 44.74, speed 169.59 f/s, eps 0.10\n","32 198 864: done 4400 games, mean reward -0.225, mean steps 45.20, speed 170.69 f/s, eps 0.10\n","32 203 902: done 4500 games, mean reward -0.223, mean steps 45.31, speed 71.98 f/s, eps 0.10\n","32 207 584: done 4600 games, mean reward -0.220, mean steps 45.13, speed 170.54 f/s, eps 0.10\n","32 212 299: done 4700 games, mean reward -0.220, mean steps 45.17, speed 170.95 f/s, eps 0.10\n","32 216 032: done 4800 games, mean reward -0.219, mean steps 45.01, speed 170.96 f/s, eps 0.10\n","32 220 921: done 4900 games, mean reward -0.221, mean steps 45.09, speed 170.66 f/s, eps 0.10\n","32 225 144: done 5000 games, mean reward -0.215, mean steps 45.03, speed 170.45 f/s, eps 0.10\n","32 228 936: done 5100 games, mean reward -0.215, mean steps 44.89, speed 171.10 f/s, eps 0.10\n","32 234 258: done 5200 games, mean reward -0.215, mean steps 45.05, speed 170.44 f/s, eps 0.10\n","32 238 728: done 5300 games, mean reward -0.222, mean steps 45.04, speed 170.16 f/s, eps 0.10\n","32 243 199: done 5400 games, mean reward -0.225, mean steps 45.04, speed 170.82 f/s, eps 0.10\n","32 247 055: done 5500 games, mean reward -0.219, mean steps 44.92, speed 169.74 f/s, eps 0.10\n","32 251 680: done 5600 games, mean reward -0.212, mean steps 44.94, speed 170.93 f/s, eps 0.10\n","32 255 388: done 5700 games, mean reward -0.213, mean steps 44.80, speed 171.10 f/s, eps 0.10\n","32 261 090: done 5800 games, mean reward -0.210, mean steps 45.02, speed 170.33 f/s, eps 0.10\n","32 265 121: done 5900 games, mean reward -0.214, mean steps 44.94, speed 171.05 f/s, eps 0.10\n","32 269 558: done 6000 games, mean reward -0.216, mean steps 44.93, speed 170.42 f/s, eps 0.10\n","32 275 639: done 6100 games, mean reward -0.207, mean steps 45.19, speed 170.49 f/s, eps 0.10\n","32 281 299: done 6200 games, mean reward -0.197, mean steps 45.37, speed 170.61 f/s, eps 0.10\n","32 286 154: done 6300 games, mean reward -0.198, mean steps 45.42, speed 170.83 f/s, eps 0.10\n","32 290 110: done 6400 games, mean reward -0.197, mean steps 45.33, speed 170.09 f/s, eps 0.10\n","32 295 196: done 6500 games, mean reward -0.197, mean steps 45.41, speed 170.55 f/s, eps 0.10\n","32 299 912: done 6600 games, mean reward -0.197, mean steps 45.44, speed 169.73 f/s, eps 0.10\n","32 305 793: done 6700 games, mean reward -0.197, mean steps 45.64, speed 79.31 f/s, eps 0.10\n","32 311 895: done 6800 games, mean reward -0.204, mean steps 45.87, speed 170.40 f/s, eps 0.10\n","32 317 605: done 6900 games, mean reward -0.202, mean steps 46.03, speed 171.45 f/s, eps 0.10\n","32 322 474: done 7000 games, mean reward -0.204, mean steps 46.07, speed 170.09 f/s, eps 0.10\n","32 326 513: done 7100 games, mean reward -0.204, mean steps 45.99, speed 170.63 f/s, eps 0.10\n","32 330 953: done 7200 games, mean reward -0.205, mean steps 45.97, speed 171.27 f/s, eps 0.10\n","32 335 327: done 7300 games, mean reward -0.204, mean steps 45.94, speed 170.27 f/s, eps 0.10\n","32 338 550: done 7400 games, mean reward -0.205, mean steps 45.75, speed 169.15 f/s, eps 0.10\n","32 342 852: done 7500 games, mean reward -0.207, mean steps 45.71, speed 170.44 f/s, eps 0.10\n","32 346 624: done 7600 games, mean reward -0.206, mean steps 45.61, speed 170.62 f/s, eps 0.10\n","32 351 761: done 7700 games, mean reward -0.210, mean steps 45.68, speed 170.89 f/s, eps 0.10\n","32 356 296: done 7800 games, mean reward -0.213, mean steps 45.68, speed 171.10 f/s, eps 0.10\n","32 362 373: done 7900 games, mean reward -0.218, mean steps 45.87, speed 171.03 f/s, eps 0.10\n","32 367 359: done 8000 games, mean reward -0.220, mean steps 45.92, speed 171.24 f/s, eps 0.10\n","32 372 967: done 8100 games, mean reward -0.219, mean steps 46.05, speed 171.46 f/s, eps 0.10\n","32 379 544: done 8200 games, mean reward -0.219, mean steps 46.29, speed 170.52 f/s, eps 0.10\n","32 384 427: done 8300 games, mean reward -0.217, mean steps 46.32, speed 171.19 f/s, eps 0.10\n","32 390 236: done 8400 games, mean reward -0.220, mean steps 46.46, speed 170.14 f/s, eps 0.10\n","32 396 101: done 8500 games, mean reward -0.218, mean steps 46.60, speed 170.20 f/s, eps 0.10\n","32 401 840: done 8600 games, mean reward -0.218, mean steps 46.73, speed 75.04 f/s, eps 0.10\n","32 407 823: done 8700 games, mean reward -0.218, mean steps 46.88, speed 171.18 f/s, eps 0.10\n","32 414 366: done 8800 games, mean reward -0.217, mean steps 47.09, speed 170.54 f/s, eps 0.10\n","32 419 925: done 8900 games, mean reward -0.215, mean steps 47.18, speed 170.58 f/s, eps 0.10\n","32 425 988: done 9000 games, mean reward -0.217, mean steps 47.33, speed 170.43 f/s, eps 0.10\n","32 432 470: done 9100 games, mean reward -0.220, mean steps 47.52, speed 170.50 f/s, eps 0.10\n","32 438 565: done 9200 games, mean reward -0.219, mean steps 47.67, speed 169.49 f/s, eps 0.10\n","32 444 454: done 9300 games, mean reward -0.220, mean steps 47.79, speed 170.88 f/s, eps 0.10\n","32 450 771: done 9400 games, mean reward -0.222, mean steps 47.95, speed 170.83 f/s, eps 0.10\n","32 456 368: done 9500 games, mean reward -0.223, mean steps 48.04, speed 170.46 f/s, eps 0.10\n","32 462 618: done 9600 games, mean reward -0.226, mean steps 48.19, speed 170.55 f/s, eps 0.10\n","32 466 915: done 9700 games, mean reward -0.224, mean steps 48.14, speed 170.39 f/s, eps 0.10\n","32 471 491: done 9800 games, mean reward -0.222, mean steps 48.11, speed 171.06 f/s, eps 0.10\n","32 474 458: done 9900 games, mean reward -0.222, mean steps 47.92, speed 171.06 f/s, eps 0.10\n","32 478 634: done 10000 games, mean reward -0.222, mean steps 47.86, speed 171.05 f/s, eps 0.10\n","32 484 218: done 10100 games, mean reward -0.225, mean steps 48.06, speed 171.03 f/s, eps 0.10\n","32 489 238: done 10200 games, mean reward -0.226, mean steps 48.27, speed 170.10 f/s, eps 0.10\n","32 493 931: done 10300 games, mean reward -0.225, mean steps 48.38, speed 170.85 f/s, eps 0.10\n","32 498 506: done 10400 games, mean reward -0.229, mean steps 48.16, speed 171.44 f/s, eps 0.10\n","32 502 136: done 10500 games, mean reward -0.230, mean steps 47.95, speed 54.99 f/s, eps 0.10\n","32 507 588: done 10600 games, mean reward -0.228, mean steps 48.03, speed 170.43 f/s, eps 0.10\n","32 511 157: done 10700 games, mean reward -0.227, mean steps 48.05, speed 170.81 f/s, eps 0.10\n","32 515 788: done 10800 games, mean reward -0.225, mean steps 48.00, speed 170.68 f/s, eps 0.10\n","32 519 146: done 10900 games, mean reward -0.223, mean steps 47.91, speed 170.64 f/s, eps 0.10\n","32 523 481: done 11000 games, mean reward -0.220, mean steps 47.81, speed 170.78 f/s, eps 0.10\n","32 528 362: done 11100 games, mean reward -0.221, mean steps 47.83, speed 170.46 f/s, eps 0.10\n","32 533 871: done 11200 games, mean reward -0.223, mean steps 47.95, speed 170.16 f/s, eps 0.10\n","32 540 080: done 11300 games, mean reward -0.220, mean steps 48.11, speed 169.76 f/s, eps 0.10\n","32 543 612: done 11400 games, mean reward -0.220, mean steps 47.89, speed 171.25 f/s, eps 0.10\n","32 549 088: done 11500 games, mean reward -0.220, mean steps 47.99, speed 170.54 f/s, eps 0.10\n","32 554 269: done 11600 games, mean reward -0.218, mean steps 48.00, speed 170.15 f/s, eps 0.10\n","32 560 456: done 11700 games, mean reward -0.216, mean steps 48.09, speed 170.06 f/s, eps 0.10\n","32 564 423: done 11800 games, mean reward -0.216, mean steps 47.90, speed 169.29 f/s, eps 0.10\n","32 570 126: done 11900 games, mean reward -0.217, mean steps 48.10, speed 168.93 f/s, eps 0.10\n","32 574 989: done 12000 games, mean reward -0.222, mean steps 48.10, speed 170.44 f/s, eps 0.10\n","32 577 726: done 12100 games, mean reward -0.223, mean steps 47.95, speed 169.63 f/s, eps 0.10\n","32 582 646: done 12200 games, mean reward -0.221, mean steps 47.84, speed 169.61 f/s, eps 0.10\n","32 586 940: done 12300 games, mean reward -0.222, mean steps 47.94, speed 168.77 f/s, eps 0.10\n","32 591 334: done 12400 games, mean reward -0.223, mean steps 47.92, speed 169.13 f/s, eps 0.10\n","32 597 044: done 12500 games, mean reward -0.224, mean steps 48.17, speed 169.61 f/s, eps 0.10\n","32 601 842: done 12600 games, mean reward -0.221, mean steps 48.26, speed 70.18 f/s, eps 0.10\n","32 607 104: done 12700 games, mean reward -0.223, mean steps 48.38, speed 170.54 f/s, eps 0.10\n","32 612 506: done 12800 games, mean reward -0.226, mean steps 48.51, speed 169.97 f/s, eps 0.10\n","32 618 130: done 12900 games, mean reward -0.222, mean steps 48.55, speed 170.13 f/s, eps 0.10\n","32 623 192: done 13000 games, mean reward -0.225, mean steps 48.62, speed 170.52 f/s, eps 0.10\n","32 629 015: done 13100 games, mean reward -0.229, mean steps 48.69, speed 170.68 f/s, eps 0.10\n","32 632 710: done 13200 games, mean reward -0.229, mean steps 48.65, speed 169.71 f/s, eps 0.10\n","32 636 748: done 13300 games, mean reward -0.224, mean steps 48.53, speed 169.79 f/s, eps 0.10\n","32 640 726: done 13400 games, mean reward -0.223, mean steps 48.56, speed 171.28 f/s, eps 0.10\n","32 644 284: done 13500 games, mean reward -0.219, mean steps 48.55, speed 170.82 f/s, eps 0.10\n","32 648 249: done 13600 games, mean reward -0.219, mean steps 48.65, speed 171.19 f/s, eps 0.10\n","32 651 954: done 13700 games, mean reward -0.219, mean steps 48.61, speed 171.41 f/s, eps 0.10\n","32 655 240: done 13800 games, mean reward -0.217, mean steps 48.52, speed 170.16 f/s, eps 0.10\n","32 659 040: done 13900 games, mean reward -0.215, mean steps 48.56, speed 170.49 f/s, eps 0.10\n","32 663 429: done 14000 games, mean reward -0.215, mean steps 48.56, speed 171.07 f/s, eps 0.10\n","32 667 190: done 14100 games, mean reward -0.214, mean steps 48.44, speed 170.44 f/s, eps 0.10\n","32 671 329: done 14200 games, mean reward -0.214, mean steps 48.40, speed 169.85 f/s, eps 0.10\n","32 675 528: done 14300 games, mean reward -0.212, mean steps 48.31, speed 171.27 f/s, eps 0.10\n","32 679 045: done 14400 games, mean reward -0.208, mean steps 48.02, speed 171.44 f/s, eps 0.10\n","32 684 035: done 14500 games, mean reward -0.207, mean steps 48.01, speed 170.38 f/s, eps 0.10\n","32 688 129: done 14600 games, mean reward -0.208, mean steps 48.05, speed 169.58 f/s, eps 0.10\n","32 692 407: done 14700 games, mean reward -0.208, mean steps 48.01, speed 171.42 f/s, eps 0.10\n","32 697 946: done 14800 games, mean reward -0.206, mean steps 48.19, speed 170.77 f/s, eps 0.10\n","32 703 202: done 14900 games, mean reward -0.208, mean steps 48.23, speed 72.07 f/s, eps 0.10\n","32 708 602: done 15000 games, mean reward -0.212, mean steps 48.35, speed 170.82 f/s, eps 0.10\n","32 713 660: done 15100 games, mean reward -0.213, mean steps 48.47, speed 170.85 f/s, eps 0.10\n","32 717 557: done 15200 games, mean reward -0.212, mean steps 48.33, speed 171.16 f/s, eps 0.10\n","32 722 116: done 15300 games, mean reward -0.207, mean steps 48.34, speed 170.50 f/s, eps 0.10\n","32 727 278: done 15400 games, mean reward -0.210, mean steps 48.41, speed 170.99 f/s, eps 0.10\n","32 730 976: done 15500 games, mean reward -0.214, mean steps 48.39, speed 170.39 f/s, eps 0.10\n","32 736 765: done 15600 games, mean reward -0.213, mean steps 48.51, speed 170.24 f/s, eps 0.10\n","32 741 520: done 15700 games, mean reward -0.210, mean steps 48.61, speed 170.97 f/s, eps 0.10\n","32 747 110: done 15800 games, mean reward -0.209, mean steps 48.60, speed 170.51 f/s, eps 0.10\n","32 751 870: done 15900 games, mean reward -0.206, mean steps 48.67, speed 170.95 f/s, eps 0.10\n","32 757 157: done 16000 games, mean reward -0.208, mean steps 48.76, speed 170.65 f/s, eps 0.10\n","32 761 768: done 16100 games, mean reward -0.214, mean steps 48.61, speed 170.98 f/s, eps 0.10\n","32 765 758: done 16200 games, mean reward -0.222, mean steps 48.45, speed 171.01 f/s, eps 0.10\n","32 770 321: done 16300 games, mean reward -0.220, mean steps 48.42, speed 170.57 f/s, eps 0.10\n","32 775 848: done 16400 games, mean reward -0.221, mean steps 48.57, speed 170.53 f/s, eps 0.10\n","32 779 908: done 16500 games, mean reward -0.224, mean steps 48.47, speed 171.05 f/s, eps 0.10\n","32 784 035: done 16600 games, mean reward -0.224, mean steps 48.41, speed 169.45 f/s, eps 0.10\n","32 789 429: done 16700 games, mean reward -0.228, mean steps 48.36, speed 170.03 f/s, eps 0.10\n","32 793 194: done 16800 games, mean reward -0.225, mean steps 48.13, speed 170.04 f/s, eps 0.10\n","32 797 278: done 16900 games, mean reward -0.228, mean steps 47.97, speed 171.10 f/s, eps 0.10\n","32 801 052: done 17000 games, mean reward -0.226, mean steps 47.86, speed 90.81 f/s, eps 0.10\n","32 804 334: done 17100 games, mean reward -0.227, mean steps 47.78, speed 171.05 f/s, eps 0.10\n","32 808 750: done 17200 games, mean reward -0.224, mean steps 47.78, speed 170.97 f/s, eps 0.10\n","32 813 630: done 17300 games, mean reward -0.223, mean steps 47.83, speed 170.96 f/s, eps 0.10\n","32 820 108: done 17400 games, mean reward -0.221, mean steps 48.16, speed 171.18 f/s, eps 0.10\n","32 825 860: done 17500 games, mean reward -0.218, mean steps 48.30, speed 170.45 f/s, eps 0.10\n","32 831 494: done 17600 games, mean reward -0.219, mean steps 48.49, speed 171.07 f/s, eps 0.10\n","32 837 485: done 17700 games, mean reward -0.214, mean steps 48.57, speed 170.02 f/s, eps 0.10\n","32 843 052: done 17800 games, mean reward -0.218, mean steps 48.68, speed 170.61 f/s, eps 0.10\n","32 848 739: done 17900 games, mean reward -0.216, mean steps 48.64, speed 170.89 f/s, eps 0.10\n","32 854 055: done 18000 games, mean reward -0.213, mean steps 48.67, speed 170.14 f/s, eps 0.10\n","32 859 463: done 18100 games, mean reward -0.215, mean steps 48.65, speed 171.60 f/s, eps 0.10\n","32 864 885: done 18200 games, mean reward -0.217, mean steps 48.53, speed 171.04 f/s, eps 0.10\n","32 869 495: done 18300 games, mean reward -0.216, mean steps 48.51, speed 170.63 f/s, eps 0.10\n","32 874 219: done 18400 games, mean reward -0.213, mean steps 48.40, speed 170.91 f/s, eps 0.10\n","32 879 654: done 18500 games, mean reward -0.219, mean steps 48.36, speed 170.41 f/s, eps 0.10\n","32 883 945: done 18600 games, mean reward -0.220, mean steps 48.21, speed 171.06 f/s, eps 0.10\n","32 888 689: done 18700 games, mean reward -0.220, mean steps 48.09, speed 169.04 f/s, eps 0.10\n","32 892 649: done 18800 games, mean reward -0.221, mean steps 47.83, speed 171.00 f/s, eps 0.10\n","32 898 207: done 18900 games, mean reward -0.224, mean steps 47.83, speed 170.71 f/s, eps 0.10\n","32 902 614: done 19000 games, mean reward -0.226, mean steps 47.66, speed 66.83 f/s, eps 0.10\n","32 907 252: done 19100 games, mean reward -0.224, mean steps 47.48, speed 171.01 f/s, eps 0.10\n","32 913 519: done 19200 games, mean reward -0.228, mean steps 47.50, speed 169.90 f/s, eps 0.10\n","32 918 143: done 19300 games, mean reward -0.228, mean steps 47.37, speed 169.93 f/s, eps 0.10\n","32 923 984: done 19400 games, mean reward -0.226, mean steps 47.32, speed 170.12 f/s, eps 0.10\n","32 928 282: done 19500 games, mean reward -0.226, mean steps 47.19, speed 169.19 f/s, eps 0.10\n","32 933 321: done 19600 games, mean reward -0.224, mean steps 47.07, speed 167.81 f/s, eps 0.10\n","32 938 351: done 19700 games, mean reward -0.229, mean steps 47.14, speed 169.42 f/s, eps 0.10\n","32 944 781: done 19800 games, mean reward -0.236, mean steps 47.33, speed 169.18 f/s, eps 0.10\n","32 950 587: done 19900 games, mean reward -0.234, mean steps 47.61, speed 169.30 f/s, eps 0.10\n","32 956 632: done 20000 games, mean reward -0.235, mean steps 47.80, speed 169.37 f/s, eps 0.10\n","32 962 508: done 20100 games, mean reward -0.231, mean steps 47.83, speed 169.49 f/s, eps 0.10\n","32 968 113: done 20200 games, mean reward -0.227, mean steps 47.89, speed 169.31 f/s, eps 0.10\n","32 974 286: done 20300 games, mean reward -0.230, mean steps 48.04, speed 170.01 f/s, eps 0.10\n","32 980 338: done 20400 games, mean reward -0.227, mean steps 48.18, speed 170.20 f/s, eps 0.10\n","32 985 571: done 20500 games, mean reward -0.229, mean steps 48.34, speed 168.61 f/s, eps 0.10\n","32 992 305: done 20600 games, mean reward -0.227, mean steps 48.47, speed 169.90 f/s, eps 0.10\n","32 996 489: done 20700 games, mean reward -0.230, mean steps 48.53, speed 169.33 f/s, eps 0.10\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type DQNConv1D. It won't be checked for correctness upon loading.\n","  \"type \" + obj.__name__ + \". It won't be checked \"\n","/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Sequential. It won't be checked for correctness upon loading.\n","  \"type \" + obj.__name__ + \". It won't be checked \"\n","/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Conv1d. It won't be checked for correctness upon loading.\n","  \"type \" + obj.__name__ + \". It won't be checked \"\n","/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ReLU. It won't be checked for correctness upon loading.\n","  \"type \" + obj.__name__ + \". It won't be checked \"\n","/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.\n","  \"type \" + obj.__name__ + \". It won't be checked \"\n"],"name":"stderr"},{"output_type":"stream","text":["33 001 481: done 20800 games, mean reward -0.231, mean steps 48.57, speed 68.19 f/s, eps 0.10\n","33 007 118: done 20900 games, mean reward -0.232, mean steps 48.80, speed 164.82 f/s, eps 0.10\n","33 012 202: done 21000 games, mean reward -0.232, mean steps 48.87, speed 168.03 f/s, eps 0.10\n","33 017 367: done 21100 games, mean reward -0.231, mean steps 48.90, speed 168.84 f/s, eps 0.10\n","33 021 470: done 21200 games, mean reward -0.233, mean steps 48.76, speed 169.02 f/s, eps 0.10\n","33 027 812: done 21300 games, mean reward -0.236, mean steps 48.77, speed 168.06 f/s, eps 0.10\n","33 031 809: done 21400 games, mean reward -0.234, mean steps 48.82, speed 166.66 f/s, eps 0.10\n","33 037 349: done 21500 games, mean reward -0.235, mean steps 48.83, speed 168.58 f/s, eps 0.10\n","33 042 464: done 21600 games, mean reward -0.238, mean steps 48.82, speed 167.88 f/s, eps 0.10\n","33 047 005: done 21700 games, mean reward -0.237, mean steps 48.65, speed 167.91 f/s, eps 0.10\n","33 052 017: done 21800 games, mean reward -0.236, mean steps 48.76, speed 168.41 f/s, eps 0.10\n","33 057 925: done 21900 games, mean reward -0.234, mean steps 48.78, speed 168.42 f/s, eps 0.10\n","33 062 124: done 22000 games, mean reward -0.235, mean steps 48.71, speed 169.01 f/s, eps 0.10\n","33 066 505: done 22100 games, mean reward -0.236, mean steps 48.88, speed 169.66 f/s, eps 0.10\n","33 072 206: done 22200 games, mean reward -0.234, mean steps 48.96, speed 169.45 f/s, eps 0.10\n","33 078 473: done 22300 games, mean reward -0.230, mean steps 49.15, speed 169.81 f/s, eps 0.10\n","33 083 982: done 22400 games, mean reward -0.231, mean steps 49.26, speed 168.61 f/s, eps 0.10\n","33 088 451: done 22500 games, mean reward -0.230, mean steps 49.14, speed 170.41 f/s, eps 0.10\n","33 092 668: done 22600 games, mean reward -0.232, mean steps 49.08, speed 170.42 f/s, eps 0.10\n","33 098 132: done 22700 games, mean reward -0.232, mean steps 49.10, speed 169.93 f/s, eps 0.10\n","33 104 094: done 22800 games, mean reward -0.233, mean steps 49.16, speed 102.96 f/s, eps 0.10\n","33 109 112: done 22900 games, mean reward -0.235, mean steps 49.10, speed 170.34 f/s, eps 0.10\n","33 114 200: done 23000 games, mean reward -0.235, mean steps 49.10, speed 170.95 f/s, eps 0.10\n","33 120 952: done 23100 games, mean reward -0.235, mean steps 49.19, speed 170.84 f/s, eps 0.10\n","33 124 653: done 23200 games, mean reward -0.235, mean steps 49.19, speed 170.67 f/s, eps 0.10\n","33 129 118: done 23300 games, mean reward -0.240, mean steps 49.24, speed 170.47 f/s, eps 0.10\n","33 132 578: done 23400 games, mean reward -0.242, mean steps 49.19, speed 169.57 f/s, eps 0.10\n","33 136 143: done 23500 games, mean reward -0.245, mean steps 49.19, speed 169.53 f/s, eps 0.10\n","33 139 350: done 23600 games, mean reward -0.247, mean steps 49.11, speed 171.19 f/s, eps 0.10\n","33 143 706: done 23700 games, mean reward -0.248, mean steps 49.18, speed 171.24 f/s, eps 0.10\n","33 148 055: done 23800 games, mean reward -0.249, mean steps 49.28, speed 169.97 f/s, eps 0.10\n","33 152 144: done 23900 games, mean reward -0.248, mean steps 49.31, speed 170.78 f/s, eps 0.10\n","33 155 867: done 24000 games, mean reward -0.245, mean steps 49.24, speed 170.74 f/s, eps 0.10\n","33 159 335: done 24100 games, mean reward -0.247, mean steps 49.21, speed 170.50 f/s, eps 0.10\n","33 165 021: done 24200 games, mean reward -0.244, mean steps 49.37, speed 170.20 f/s, eps 0.10\n","33 169 031: done 24300 games, mean reward -0.245, mean steps 49.35, speed 170.84 f/s, eps 0.10\n","33 172 685: done 24400 games, mean reward -0.247, mean steps 49.36, speed 171.34 f/s, eps 0.10\n","33 177 781: done 24500 games, mean reward -0.246, mean steps 49.37, speed 169.50 f/s, eps 0.10\n","33 183 164: done 24600 games, mean reward -0.247, mean steps 49.50, speed 170.40 f/s, eps 0.10\n","33 188 527: done 24700 games, mean reward -0.250, mean steps 49.61, speed 169.19 f/s, eps 0.10\n","33 194 007: done 24800 games, mean reward -0.250, mean steps 49.61, speed 169.66 f/s, eps 0.10\n","33 199 963: done 24900 games, mean reward -0.248, mean steps 49.68, speed 170.77 f/s, eps 0.10\n","33 205 454: done 25000 games, mean reward -0.250, mean steps 49.69, speed 71.55 f/s, eps 0.10\n","33 210 054: done 25100 games, mean reward -0.248, mean steps 49.64, speed 169.68 f/s, eps 0.10\n","33 214 831: done 25200 games, mean reward -0.249, mean steps 49.73, speed 170.51 f/s, eps 0.10\n","33 219 100: done 25300 games, mean reward -0.250, mean steps 49.70, speed 170.78 f/s, eps 0.10\n","33 223 527: done 25400 games, mean reward -0.246, mean steps 49.62, speed 169.53 f/s, eps 0.10\n","33 226 860: done 25500 games, mean reward -0.246, mean steps 49.59, speed 170.27 f/s, eps 0.10\n","33 231 373: done 25600 games, mean reward -0.251, mean steps 49.46, speed 169.29 f/s, eps 0.10\n","33 234 672: done 25700 games, mean reward -0.254, mean steps 49.32, speed 170.86 f/s, eps 0.10\n","33 239 710: done 25800 games, mean reward -0.257, mean steps 49.26, speed 170.02 f/s, eps 0.10\n","33 245 224: done 25900 games, mean reward -0.259, mean steps 49.34, speed 170.80 f/s, eps 0.10\n","33 250 483: done 26000 games, mean reward -0.258, mean steps 49.33, speed 170.32 f/s, eps 0.10\n","33 254 316: done 26100 games, mean reward -0.259, mean steps 49.25, speed 170.97 f/s, eps 0.10\n","33 258 932: done 26200 games, mean reward -0.258, mean steps 49.32, speed 171.48 f/s, eps 0.10\n","33 262 930: done 26300 games, mean reward -0.259, mean steps 49.26, speed 170.58 f/s, eps 0.10\n","33 267 173: done 26400 games, mean reward -0.258, mean steps 49.13, speed 170.15 f/s, eps 0.10\n","33 272 994: done 26500 games, mean reward -0.259, mean steps 49.31, speed 170.55 f/s, eps 0.10\n","33 277 903: done 26600 games, mean reward -0.258, mean steps 49.39, speed 170.28 f/s, eps 0.10\n","33 282 841: done 26700 games, mean reward -0.259, mean steps 49.34, speed 170.34 f/s, eps 0.10\n","33 288 211: done 26800 games, mean reward -0.257, mean steps 49.50, speed 170.55 f/s, eps 0.10\n","33 292 908: done 26900 games, mean reward -0.256, mean steps 49.56, speed 170.71 f/s, eps 0.10\n","33 296 984: done 27000 games, mean reward -0.255, mean steps 49.59, speed 170.87 f/s, eps 0.10\n","33 302 125: done 27100 games, mean reward -0.255, mean steps 49.78, speed 69.71 f/s, eps 0.10\n","33 308 199: done 27200 games, mean reward -0.257, mean steps 49.94, speed 170.76 f/s, eps 0.10\n","33 313 029: done 27300 games, mean reward -0.261, mean steps 49.94, speed 170.41 f/s, eps 0.10\n","33 318 318: done 27400 games, mean reward -0.264, mean steps 49.82, speed 170.07 f/s, eps 0.10\n","33 322 998: done 27500 games, mean reward -0.260, mean steps 49.71, speed 170.90 f/s, eps 0.10\n","33 326 558: done 27600 games, mean reward -0.260, mean steps 49.51, speed 170.21 f/s, eps 0.10\n","33 331 272: done 27700 games, mean reward -0.263, mean steps 49.38, speed 169.21 f/s, eps 0.10\n","33 335 841: done 27800 games, mean reward -0.261, mean steps 49.28, speed 170.91 f/s, eps 0.10\n","33 339 567: done 27900 games, mean reward -0.257, mean steps 49.08, speed 170.26 f/s, eps 0.10\n","33 344 495: done 28000 games, mean reward -0.260, mean steps 49.04, speed 170.75 f/s, eps 0.10\n","33 349 993: done 28100 games, mean reward -0.255, mean steps 49.05, speed 170.57 f/s, eps 0.10\n","33 354 914: done 28200 games, mean reward -0.254, mean steps 49.00, speed 170.40 f/s, eps 0.10\n","33 360 332: done 28300 games, mean reward -0.254, mean steps 49.08, speed 170.08 f/s, eps 0.10\n","33 365 433: done 28400 games, mean reward -0.255, mean steps 49.12, speed 169.31 f/s, eps 0.10\n","33 370 884: done 28500 games, mean reward -0.252, mean steps 49.12, speed 170.60 f/s, eps 0.10\n","33 374 599: done 28600 games, mean reward -0.250, mean steps 49.07, speed 170.91 f/s, eps 0.10\n","33 378 771: done 28700 games, mean reward -0.247, mean steps 49.01, speed 170.80 f/s, eps 0.10\n","33 382 989: done 28800 games, mean reward -0.247, mean steps 49.03, speed 169.35 f/s, eps 0.10\n","33 387 500: done 28900 games, mean reward -0.244, mean steps 48.93, speed 170.72 f/s, eps 0.10\n","33 391 925: done 29000 games, mean reward -0.241, mean steps 48.93, speed 170.55 f/s, eps 0.10\n","33 394 915: done 29100 games, mean reward -0.240, mean steps 48.77, speed 169.41 f/s, eps 0.10\n","33 399 072: done 29200 games, mean reward -0.236, mean steps 48.56, speed 170.32 f/s, eps 0.10\n","33 402 964: done 29300 games, mean reward -0.234, mean steps 48.48, speed 60.44 f/s, eps 0.10\n","33 407 389: done 29400 games, mean reward -0.235, mean steps 48.34, speed 170.20 f/s, eps 0.10\n","33 411 093: done 29500 games, mean reward -0.234, mean steps 48.28, speed 170.14 f/s, eps 0.10\n","33 415 517: done 29600 games, mean reward -0.233, mean steps 48.22, speed 169.32 f/s, eps 0.10\n","33 420 479: done 29700 games, mean reward -0.234, mean steps 48.21, speed 170.55 f/s, eps 0.10\n","33 426 232: done 29800 games, mean reward -0.225, mean steps 48.15, speed 169.97 f/s, eps 0.10\n","33 431 577: done 29900 games, mean reward -0.228, mean steps 48.10, speed 169.61 f/s, eps 0.10\n","33 435 988: done 30000 games, mean reward -0.227, mean steps 47.94, speed 170.69 f/s, eps 0.10\n","33 439 125: done 30100 games, mean reward -0.227, mean steps 47.66, speed 170.02 f/s, eps 0.10\n","33 442 940: done 30200 games, mean reward -0.231, mean steps 47.48, speed 169.94 f/s, eps 0.10\n","33 446 732: done 30300 games, mean reward -0.229, mean steps 47.24, speed 170.23 f/s, eps 0.10\n","33 450 958: done 30400 games, mean reward -0.230, mean steps 47.06, speed 170.86 f/s, eps 0.10\n","33 455 025: done 30500 games, mean reward -0.228, mean steps 46.95, speed 170.07 f/s, eps 0.10\n","33 460 831: done 30600 games, mean reward -0.231, mean steps 46.85, speed 170.60 f/s, eps 0.10\n","33 466 034: done 30700 games, mean reward -0.235, mean steps 46.95, speed 170.02 f/s, eps 0.10\n","33 472 806: done 30800 games, mean reward -0.232, mean steps 47.13, speed 170.02 f/s, eps 0.10\n","33 478 237: done 30900 games, mean reward -0.233, mean steps 47.11, speed 170.37 f/s, eps 0.10\n","33 481 562: done 31000 games, mean reward -0.232, mean steps 46.94, speed 169.52 f/s, eps 0.10\n","33 486 453: done 31100 games, mean reward -0.234, mean steps 46.91, speed 170.48 f/s, eps 0.10\n","33 492 489: done 31200 games, mean reward -0.233, mean steps 47.10, speed 170.53 f/s, eps 0.10\n","33 497 517: done 31300 games, mean reward -0.229, mean steps 46.97, speed 170.24 f/s, eps 0.10\n","33 501 245: done 31400 games, mean reward -0.229, mean steps 46.94, speed 58.65 f/s, eps 0.10\n","33 507 793: done 31500 games, mean reward -0.230, mean steps 47.04, speed 170.69 f/s, eps 0.10\n","33 513 142: done 31600 games, mean reward -0.228, mean steps 47.07, speed 170.10 f/s, eps 0.10\n","33 517 031: done 31700 games, mean reward -0.231, mean steps 47.00, speed 169.90 f/s, eps 0.10\n","33 521 501: done 31800 games, mean reward -0.234, mean steps 46.95, speed 170.70 f/s, eps 0.10\n","33 526 534: done 31900 games, mean reward -0.233, mean steps 46.86, speed 169.80 f/s, eps 0.10\n","33 530 876: done 32000 games, mean reward -0.233, mean steps 46.88, speed 170.60 f/s, eps 0.10\n","33 534 440: done 32100 games, mean reward -0.230, mean steps 46.79, speed 170.83 f/s, eps 0.10\n","33 539 511: done 32200 games, mean reward -0.232, mean steps 46.73, speed 170.99 f/s, eps 0.10\n","33 544 300: done 32300 games, mean reward -0.236, mean steps 46.58, speed 170.08 f/s, eps 0.10\n","33 549 495: done 32400 games, mean reward -0.234, mean steps 46.55, speed 171.08 f/s, eps 0.10\n","33 553 945: done 32500 games, mean reward -0.233, mean steps 46.55, speed 170.88 f/s, eps 0.10\n","33 558 373: done 32600 games, mean reward -0.236, mean steps 46.57, speed 169.58 f/s, eps 0.10\n","33 563 659: done 32700 games, mean reward -0.231, mean steps 46.55, speed 169.88 f/s, eps 0.10\n","33 567 021: done 32800 games, mean reward -0.230, mean steps 46.29, speed 170.82 f/s, eps 0.10\n","33 570 767: done 32900 games, mean reward -0.232, mean steps 46.17, speed 170.53 f/s, eps 0.10\n","33 574 277: done 33000 games, mean reward -0.229, mean steps 46.01, speed 169.95 f/s, eps 0.10\n","33 578 910: done 33100 games, mean reward -0.229, mean steps 45.80, speed 169.18 f/s, eps 0.10\n","33 582 786: done 33200 games, mean reward -0.229, mean steps 45.81, speed 170.23 f/s, eps 0.10\n","33 586 746: done 33300 games, mean reward -0.228, mean steps 45.76, speed 169.40 f/s, eps 0.10\n","33 590 948: done 33400 games, mean reward -0.228, mean steps 45.84, speed 170.21 f/s, eps 0.10\n","33 596 683: done 33500 games, mean reward -0.229, mean steps 46.05, speed 170.71 f/s, eps 0.10\n","33 602 047: done 33600 games, mean reward -0.226, mean steps 46.27, speed 101.19 f/s, eps 0.10\n","33 605 948: done 33700 games, mean reward -0.224, mean steps 46.22, speed 171.02 f/s, eps 0.10\n","33 610 105: done 33800 games, mean reward -0.224, mean steps 46.21, speed 169.94 f/s, eps 0.10\n","33 614 939: done 33900 games, mean reward -0.227, mean steps 46.28, speed 170.53 f/s, eps 0.10\n","33 620 063: done 34000 games, mean reward -0.231, mean steps 46.42, speed 170.62 f/s, eps 0.10\n","33 625 373: done 34100 games, mean reward -0.229, mean steps 46.60, speed 170.08 f/s, eps 0.10\n","33 629 957: done 34200 games, mean reward -0.232, mean steps 46.49, speed 169.66 f/s, eps 0.10\n","33 632 854: done 34300 games, mean reward -0.232, mean steps 46.38, speed 169.99 f/s, eps 0.10\n","33 638 775: done 34400 games, mean reward -0.235, mean steps 46.61, speed 170.28 f/s, eps 0.10\n","33 642 337: done 34500 games, mean reward -0.239, mean steps 46.46, speed 170.45 f/s, eps 0.10\n","33 645 502: done 34600 games, mean reward -0.238, mean steps 46.23, speed 168.52 f/s, eps 0.10\n","33 648 947: done 34700 games, mean reward -0.234, mean steps 46.04, speed 170.14 f/s, eps 0.10\n","33 654 332: done 34800 games, mean reward -0.237, mean steps 46.03, speed 169.88 f/s, eps 0.10\n","33 659 424: done 34900 games, mean reward -0.237, mean steps 45.95, speed 170.56 f/s, eps 0.10\n","33 664 106: done 35000 games, mean reward -0.234, mean steps 45.87, speed 168.87 f/s, eps 0.10\n","33 668 575: done 35100 games, mean reward -0.235, mean steps 45.85, speed 170.92 f/s, eps 0.10\n","33 674 031: done 35200 games, mean reward -0.235, mean steps 45.92, speed 170.93 f/s, eps 0.10\n","33 679 046: done 35300 games, mean reward -0.238, mean steps 45.99, speed 170.45 f/s, eps 0.10\n","33 683 545: done 35400 games, mean reward -0.234, mean steps 46.00, speed 169.64 f/s, eps 0.10\n","33 688 735: done 35500 games, mean reward -0.233, mean steps 46.19, speed 170.71 f/s, eps 0.10\n","33 692 709: done 35600 games, mean reward -0.234, mean steps 46.13, speed 169.19 f/s, eps 0.10\n","33 697 844: done 35700 games, mean reward -0.233, mean steps 46.32, speed 170.11 f/s, eps 0.10\n","33 701 549: done 35800 games, mean reward -0.232, mean steps 46.18, speed 87.49 f/s, eps 0.10\n","33 706 806: done 35900 games, mean reward -0.229, mean steps 46.16, speed 170.70 f/s, eps 0.10\n","33 710 703: done 36000 games, mean reward -0.228, mean steps 46.02, speed 170.25 f/s, eps 0.10\n","33 714 086: done 36100 games, mean reward -0.226, mean steps 45.98, speed 168.78 f/s, eps 0.10\n","33 717 569: done 36200 games, mean reward -0.225, mean steps 45.86, speed 169.49 f/s, eps 0.10\n","33 721 418: done 36300 games, mean reward -0.225, mean steps 45.85, speed 169.72 f/s, eps 0.10\n","33 728 075: done 36400 games, mean reward -0.226, mean steps 46.09, speed 169.18 f/s, eps 0.10\n","33 732 284: done 36500 games, mean reward -0.223, mean steps 45.93, speed 168.57 f/s, eps 0.10\n","33 736 627: done 36600 games, mean reward -0.222, mean steps 45.87, speed 169.97 f/s, eps 0.10\n","33 740 122: done 36700 games, mean reward -0.217, mean steps 45.73, speed 170.14 f/s, eps 0.10\n","33 744 016: done 36800 games, mean reward -0.218, mean steps 45.58, speed 169.91 f/s, eps 0.10\n","33 747 724: done 36900 games, mean reward -0.219, mean steps 45.48, speed 170.34 f/s, eps 0.10\n","33 752 573: done 37000 games, mean reward -0.219, mean steps 45.56, speed 169.89 f/s, eps 0.10\n","33 756 514: done 37100 games, mean reward -0.219, mean steps 45.44, speed 169.61 f/s, eps 0.10\n","33 760 718: done 37200 games, mean reward -0.220, mean steps 45.25, speed 169.81 f/s, eps 0.10\n","33 765 429: done 37300 games, mean reward -0.217, mean steps 45.24, speed 169.26 f/s, eps 0.10\n","33 770 681: done 37400 games, mean reward -0.214, mean steps 45.24, speed 169.75 f/s, eps 0.10\n","33 773 937: done 37500 games, mean reward -0.221, mean steps 45.09, speed 169.91 f/s, eps 0.10\n","33 778 080: done 37600 games, mean reward -0.219, mean steps 45.15, speed 170.04 f/s, eps 0.10\n","33 781 834: done 37700 games, mean reward -0.217, mean steps 45.06, speed 169.61 f/s, eps 0.10\n","33 785 781: done 37800 games, mean reward -0.216, mean steps 44.99, speed 169.18 f/s, eps 0.10\n","33 789 602: done 37900 games, mean reward -0.214, mean steps 45.00, speed 170.19 f/s, eps 0.10\n","33 795 200: done 38000 games, mean reward -0.211, mean steps 45.07, speed 170.19 f/s, eps 0.10\n","33 799 273: done 38100 games, mean reward -0.212, mean steps 44.93, speed 170.30 f/s, eps 0.10\n","33 802 581: done 38200 games, mean reward -0.211, mean steps 44.77, speed 85.49 f/s, eps 0.10\n","33 807 913: done 38300 games, mean reward -0.210, mean steps 44.76, speed 170.38 f/s, eps 0.10\n","33 812 082: done 38400 games, mean reward -0.211, mean steps 44.66, speed 170.26 f/s, eps 0.10\n","33 815 492: done 38500 games, mean reward -0.209, mean steps 44.46, speed 170.12 f/s, eps 0.10\n","33 819 297: done 38600 games, mean reward -0.210, mean steps 44.47, speed 170.45 f/s, eps 0.10\n","33 823 286: done 38700 games, mean reward -0.218, mean steps 44.45, speed 170.51 f/s, eps 0.10\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-11-85cb15c7fb4f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mstep_idx\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mbuffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpopulate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         selector.epsilon = max(\n\u001b[1;32m      6\u001b[0m             \u001b[0mEPSILON_STOP\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPSILON_START\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstep_idx\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mEPSILON_STEPS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ptan/experience.py\u001b[0m in \u001b[0;36mpopulate\u001b[0;34m(self, samples)\u001b[0m\n\u001b[1;32m    366\u001b[0m         \"\"\"\n\u001b[1;32m    367\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 368\u001b[0;31m             \u001b[0mentry\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperience_source_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    369\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentry\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ptan/experience.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mexp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mExperienceSourceFirstLast\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mexp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m                 \u001b[0mlast_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ptan/experience.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     80\u001b[0m                     \u001b[0mstates_indices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstates_input\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m                 \u001b[0mstates_actions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_agent_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m                     \u001b[0mg_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstates_indices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_no_grad\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_no_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_no_grad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ptan/agent.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, states, agent_states)\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m                 \u001b[0mstates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0mq_v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdqn_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m         \u001b[0mq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq_v\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_selector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/My Drive/NTNU/6. Semester/NEURO140/agent1/lib/models.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0mconv_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m         \u001b[0mval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc_val\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0madv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc_adv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"vBEfC015ki5y","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}